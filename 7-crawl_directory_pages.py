"""
7-crawl_directory_pages.py
----------------------------------
çˆ¬å–ç‰¹å®šç›®å½•ä¸‹çš„æ‰€æœ‰é¡µé¢
ä¾‹å¦‚ï¼šåªçˆ¬å– https://www.runoob.com/regexp/ ç›®å½•ä¸‹çš„æ‰€æœ‰HTMLæ–‡ä»¶
"""
import asyncio
import os
from urllib.parse import urljoin, urlparse
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator


class DirectoryCrawler:
    """ç›®å½•çº§é¡µé¢çˆ¬å–å™¨"""
    
    def __init__(self, directory_url, max_depth=1, max_pages=50, output_dir=None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            directory_url: ç›®å½•URLï¼ˆå¦‚ https://www.runoob.com/regexp/ï¼‰
            max_depth: æœ€å¤§çˆ¬å–æ·±åº¦ï¼ˆç›¸å¯¹äºç›®å½•ï¼‰
            max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°
            output_dir: è¾“å‡ºç›®å½•
        """
        self.directory_url = directory_url.rstrip('/') + '/'
        self.max_depth = max_depth
        self.max_pages = max_pages
        
        # ç”Ÿæˆè¾“å‡ºç›®å½•å
        if output_dir is None:
            parsed = urlparse(directory_url)
            dir_name = parsed.path.strip('/').replace('/', '_') or 'root'
            self.output_dir = f"crawled_{parsed.netloc}_{dir_name}"
        else:
            self.output_dir = output_dir
        
        # çˆ¬å–çŠ¶æ€
        self.visited_urls = set()
        self.pending_urls = []
        self.crawled_pages = []
        
        # è§£æåŸºç¡€ä¿¡æ¯
        parsed = urlparse(directory_url)
        self.base_domain = parsed.netloc
        self.base_scheme = parsed.scheme
        self.directory_path = parsed.path
        
        # å‡†å¤‡è¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ¯ ç›®æ ‡ç›®å½•: {self.directory_url}")
    
    def is_in_target_directory(self, url):
        """æ£€æŸ¥URLæ˜¯å¦åœ¨ç›®æ ‡ç›®å½•ä¸‹"""
        try:
            parsed = urlparse(url)
            
            # æ£€æŸ¥åŸŸåæ˜¯å¦ç›¸åŒ
            if parsed.netloc != self.base_domain:
                return False
            
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦åœ¨ç›®æ ‡ç›®å½•ä¸‹
            url_path = parsed.path
            return url_path.startswith(self.directory_path)
        except:
            return False
    
    def normalize_url(self, url):
        """æ ‡å‡†åŒ–URL"""
        # ç§»é™¤ç‰‡æ®µæ ‡è¯†ç¬¦
        url = url.split('#')[0]
        # ç¡®ä¿å®Œæ•´URL
        if url.startswith('//'):
            url = f"{self.base_scheme}:{url}"
        elif url.startswith('/'):
            url = f"{self.base_scheme}://{self.base_domain}{url}"
        return url
    
    def get_safe_filename(self, url):
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        parsed = urlparse(url)
        path = parsed.path.strip('/') or 'index'
        
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        filename = path.replace('/', '_')
        filename = ''.join(c if c.isalnum() or c in ('_', '-', '.') else '_' for c in filename)
        
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        
        if not filename.endswith('.md'):
            filename += '.md'
            
        return filename
    
    def save_page_content(self, url, result, depth):
        """ä¿å­˜é¡µé¢å†…å®¹åˆ°æ–‡ä»¶"""
        try:
            filename = self.get_safe_filename(url)
            filepath = os.path.join(self.output_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"# Source: {url}\n")
                f.write(f"**Depth**: {depth}\n")
                f.write(f"**Crawled at**: {result.timestamp if hasattr(result, 'timestamp') else 'N/A'}\n\n")
                f.write("---\n\n")
                f.write(result.markdown.raw_markdown if result.markdown else "No content")
            
            return filepath
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥ {url}: {e}")
            return None
    
    def extract_links_from_html(self, html_content, base_url):
        """ä»HTMLå†…å®¹ä¸­æå–é“¾æ¥"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            links = []
            
            for link in soup.find_all('a', href=True):
                href = link.get('href', '').strip()
                if href and not href.startswith(('javascript:', 'mailto:', 'tel:')):
                    full_url = urljoin(base_url, href)
                    normalized_url = self.normalize_url(full_url)
                    
                    # åªä¿ç•™ç›®æ ‡ç›®å½•ä¸‹çš„é“¾æ¥
                    if self.is_in_target_directory(normalized_url):
                        links.append(normalized_url)
            
            return list(set(links))  # å»é‡
        except Exception as e:
            print(f"âŒ é“¾æ¥æå–å¤±è´¥: {e}")
            return []
    
    async def crawl_directory(self):
        """å¼€å§‹çˆ¬å–ç›®å½•ä¸‹çš„é¡µé¢"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å–ç›®å½•: {self.directory_url}")
        print(f"ğŸ“Š é…ç½®: æœ€å¤§æ·±åº¦={self.max_depth}, æœ€å¤§é¡µé¢æ•°={self.max_pages}")
        
        # åˆå§‹åŒ–çˆ¬è™«
        browser_config = BrowserConfig(headless=True)
        crawl_config = CrawlerRunConfig(
            markdown_generator=DefaultMarkdownGenerator()
        )
        
        crawler = AsyncWebCrawler(config=browser_config)
        await crawler.start()
        
        try:
            # ä»ç›®å½•é¦–é¡µå¼€å§‹
            self.pending_urls.append((self.directory_url, 0))
            
            while self.pending_urls and len(self.crawled_pages) < self.max_pages:
                url, depth = self.pending_urls.pop(0)
                
                # æ£€æŸ¥æ˜¯å¦å·²è®¿é—®
                if url in self.visited_urls:
                    continue
                
                # æ£€æŸ¥æ·±åº¦é™åˆ¶
                if depth > self.max_depth:
                    continue
                
                self.visited_urls.add(url)
                
                print(f"ğŸ” [{depth}] çˆ¬å–: {url}")
                
                # çˆ¬å–é¡µé¢
                result = await crawler.arun(url=url, config=crawl_config)
                
                if result.success:
                    print(f"âœ… æˆåŠŸ: {url}")
                    
                    # ä¿å­˜å†…å®¹
                    filepath = self.save_page_content(url, result, depth)
                    
                    # è®°å½•çˆ¬å–ç»“æœ
                    page_info = {
                        'url': url,
                        'depth': depth,
                        'filepath': filepath,
                        'markdown_length': len(result.markdown.raw_markdown) if result.markdown else 0,
                        'success': True
                    }
                    self.crawled_pages.append(page_info)
                    
                    # æå–æ–°é“¾æ¥ï¼ˆå¦‚æœæœªè¾¾åˆ°æ·±åº¦é™åˆ¶ï¼‰
                    if depth < self.max_depth and result.html:
                        new_links = self.extract_links_from_html(result.html, url)
                        
                        for link in new_links:
                            if link not in self.visited_urls and link not in [u for u, _ in self.pending_urls]:
                                self.pending_urls.append((link, depth + 1))
                        
                        print(f"ğŸ”— å‘ç° {len(new_links)} ä¸ªæ–°é“¾æ¥")
                    
                else:
                    print(f"âŒ å¤±è´¥: {url} - {result.error_message}")
                    self.crawled_pages.append({
                        'url': url,
                        'depth': depth,
                        'filepath': None,
                        'success': False,
                        'error': result.error_message
                    })
            
            # ç”ŸæˆæŠ¥å‘Š
            self.generate_report()
            
        finally:
            await crawler.close()
    
    def generate_report(self):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report_path = os.path.join(self.output_dir, "_CRAWL_REPORT.md")
        
        success_count = sum(1 for p in self.crawled_pages if p['success'])
        total_count = len(self.crawled_pages)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# ç›®å½•çˆ¬å–æŠ¥å‘Š\n\n")
            f.write(f"**ç›®æ ‡ç›®å½•**: {self.directory_url}\n")
            f.write(f"**çˆ¬å–é¡µé¢**: {len(self.crawled_pages)} ä¸ªé¡µé¢\n")
            f.write(f"**æˆåŠŸé¡µé¢**: {success_count}/{total_count}\n")
            f.write(f"**æœ€å¤§æ·±åº¦**: {self.max_depth}\n\n")
            
            f.write("## é¡µé¢åˆ—è¡¨\n\n")
            for page in self.crawled_pages:
                status = "âœ…" if page['success'] else "âŒ"
                f.write(f"{status} [{page['depth']}] {page['url']}\n")
                if page['success'] and page['filepath']:
                    f.write(f"   ğŸ“„ {page['filepath']} ({page['markdown_length']} chars)\n")
        
        print(f"ğŸ“Š çˆ¬å–å®Œæˆ! æˆåŠŸ: {success_count}/{total_count}")
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


async def main():
    """ä¸»å‡½æ•° - ä¿®æ”¹è¿™é‡Œçš„é…ç½®æ¥çˆ¬å–ä¸åŒç›®å½•"""
    
    # é…ç½®è¦çˆ¬å–çš„ç›®å½•
    directory_url = "https://www.runoob.com/git/"  # ä¿®æ”¹ä¸ºä½ è¦çˆ¬å–çš„ç›®å½•
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = DirectoryCrawler(
        directory_url=directory_url,
        max_depth=1,      # çˆ¬å–æ·±åº¦ï¼ˆç›¸å¯¹äºç›®å½•ï¼‰
        max_pages=30,     # æœ€å¤§é¡µé¢æ•°
    )
    
    # å¼€å§‹çˆ¬å–
    await crawler.crawl_directory()


if __name__ == "__main__":
    asyncio.run(main())